{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import nltk as nltk #nlp module\n",
    "import matplotlib as plot #visuals\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import spacy #nlp module\n",
    "import re\n",
    "import random\n",
    "from random import sample\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "from spacy.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the CSV and name the columns. We're working mainly with the third column - summaries about the articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textRead = pd.read_csv('factCheck.csv')\n",
    "#textRead = textRead.parse('Sheet1')\n",
    "textRead.columns = ['Json', 'Accuracy', 'Title', 'Genre', 'KeywordName', 'Occupation', 'Location', 'PoliticalParty', \n",
    "                    '1', '2', '3', '4', '5', 'Source', 'Url']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentenceFrame = textRead[textRead.columns[2]]\n",
    "#sentenceFrame = pd.DataFrame(sentenceFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the \"-ing\" verbs in the frame. Not really sure why. But here they are! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voting\n",
      "starting\n",
      "during\n",
      "spending\n",
      "making\n",
      "cutting\n",
      "Shooting\n",
      "endorsing\n",
      "going\n",
      "going\n",
      "existing\n",
      "including\n",
      "seeking\n",
      "making\n",
      "having\n",
      "going\n",
      "choosing\n",
      "being\n",
      "learning\n",
      "being\n",
      "driving\n",
      "During\n",
      "forthcoming\n",
      "manufacturing\n",
      "nothing\n",
      "fastest-growing\n",
      "kidnapping\n",
      "peacekeeping\n",
      "setting\n",
      "privatizing\n",
      "facing\n",
      "leaving\n",
      "during\n",
      "totaling\n",
      "Raising\n",
      "engaging\n",
      "opening\n",
      "going\n",
      "saying\n",
      "growing\n",
      "receiving\n",
      "looking\n",
      "creating\n",
      "serving\n",
      "spending\n",
      "getting\n",
      "creating\n",
      "bring\n",
      "amazing\n",
      "going\n",
      "bring\n",
      "arriving\n",
      "spending\n",
      "begging\n",
      "waiting\n"
     ]
    }
   ],
   "source": [
    "for line in sentenceFrame:\n",
    "    for word in line.split():\n",
    "        if word.endswith('ing'):\n",
    "            print (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:\n",
    "        if word.endswith(suffix): return word[:-len(suffix)]\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(set(sentenceFrame))\n",
    "len(set(sentenceFrame))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lexical diversity is how common most of the words are, to see if some words appear more than others. In this case, since the diversity returns 1, most of the words are unique. Nothing special. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lexical_diversity(sentenceFrame):\n",
    "    return (len(sentenceFrame)) / len(set(sentenceFrame))\n",
    "\n",
    "lexical_diversity(sentenceFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = set(sentenceFrame)\n",
    "tokens = sorted(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_features(textRead):\n",
    "    words = word_tokenize(textRead)\n",
    "    features = {}\n",
    "    for w in words_features:\n",
    "        features[w] = (w in words)\n",
    "        print (features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start manipulating the data. First, we must tokenize the data frame to differentiate between the sentences and phrases. Stop words (common words such as \"at\", \"by\", etc.) have been removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [When, did, the, decline, of, coal, start, ?, ...\n",
       "1    [Hillary, Clinton, agrees, with, John, McCain,...\n",
       "2    [Health, care, reform, legislation, is, likely...\n",
       "3    [The, economic, turnaround, started, at, the, ...\n",
       "4    [The, Chicago, Bears, have, had, more, startin...\n",
       "Name: Title, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = sentenceFrame.apply(word_tokenize)\n",
    "txt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"stop_words.txt\", 'r') as f:\n",
    "    stopwords = f.read().split(\"\\n\")\n",
    "\n",
    "filtered_sentence = [w for w in txt if not w in stopwords]\n",
    "\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in txt:\n",
    "    if w not in stopwords:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "#print(txt)\n",
    "#print(filtered_sentence)\n",
    "filtered_sentence = pd.DataFrame(filtered_sentence)\n",
    "#filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am experimenting with different ways to create training and test sets. None are necessarily wrong, but I think the simplest way is to just subset. See line 166. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_sets(filtered_sentence, test_portion):\n",
    "    tot_ix = range(len(filtered_sentence))\n",
    "    test_ix = sorted(random.sample(tot_ix, int(test_portion * len(filtered_sentence))))\n",
    "    train_ix = list(set(tot_ix) ^ set(test_ix))\n",
    "\n",
    "    test_df = filtered_sentence.ix[test_ix]\n",
    "    train_df = filtered_sentence.ix[train_ix]\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "train_df, test_df = make_sets(filtered_sentence, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traindf = train_df.all()\n",
    "testdf = test_df.all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#custom_sent_tokenizer = PunktSentenceTokenizer(traindf)\n",
    "#tokenized = custom_sent_tokenizer.tokenize(testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = filtered_sentence[:101]\n",
    "test = filtered_sentence[101:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to try to apply the algorithm. We are going to use the Naive Bayes classification algorithm. \n",
    "This is a pretty popular algorithm used in text classification, implementing supervised machine learning. We're showing the machine data, and telling it \"hey, this data is positive,\" or \"this data is negative.\" Then, after that training is done, we show the machine some new data and ask the computer, based on what we taught the computer before, what the computer thinks the category of the new data is.\n",
    "\n",
    "I get stuck here. Why isn't my dataframe iterable? Any suggestions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.int64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-b472f8d64c68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/samanthalee/anaconda/lib/python3.5/site-packages/nltk/classify/naivebayes.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, labeled_featuresets, estimator)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# Count up how many times each feature value occurred, given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;31m# the label and featurename.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabeled_featuresets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mlabel_freqdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.int64' object is not iterable"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
